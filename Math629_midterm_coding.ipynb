{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "142c7ea7",
   "metadata": {
    "id": "142c7ea7"
   },
   "source": [
    "**You are free to consult any source online, for syntax or conceptual understanding or for any other help you need but please write your own code**\n",
    "\n",
    "In Problem 2, don't worry if your results are bad or so, we are mainly looking at your approach to handle a new problem. Just put in your best effort and you will be fine, if you are using google colab, convert that into notebook and then submit it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ab29b2",
   "metadata": {
    "id": "11ab29b2"
   },
   "source": [
    "# Problem 1( 30 Points )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22e2f02",
   "metadata": {
    "id": "b22e2f02"
   },
   "source": [
    "Till now in the course, we have looked at Pytorch for doing deep learning. However, it is important to know how neural networks are implemented from scratch because it helps a lot in doing debugging and it increases your fundamental understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400857c3",
   "metadata": {
    "id": "400857c3"
   },
   "source": [
    "In this problem, you will implement a very simple three layer neural network with Input $x$, $W_1, W_2, W_3$ be the weights and $b_1, b_2, b_3$ be the biases and $y$ be the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c420c8bd",
   "metadata": {
    "id": "c420c8bd"
   },
   "source": [
    "\n",
    "![Image](https://drive.google.com/uc?id=1TW85VcYzDNwTTWDKEOR1fSEAIw3Rtw5V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc1764a",
   "metadata": {
    "id": "1fc1764a"
   },
   "source": [
    "We will work with BodyFat Data. Use 150 examples for training and the remainder  for  testing  with  mean  squared  error.  Activation  functions are rectified linear unit (ReLU) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cc168a",
   "metadata": {
    "id": "94cc168a"
   },
   "source": [
    "You have to do the following:-\n",
    "\n",
    "a) Implement the forward pass <br>\n",
    "b) Implement the backward pass <br>\n",
    "c) Submit code and report on mean square error on training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d66c3c",
   "metadata": {
    "id": "88d66c3c"
   },
   "source": [
    "# Notes:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5778a398",
   "metadata": {
    "id": "5778a398"
   },
   "source": [
    "1) seed : np.random.seed(0) <br>\n",
    "2) Initialize weights following a standard normal distribution <br>\n",
    "3) Initialize biases with zeros <br>\n",
    "3) First layer 64 hidden units, second layer 16 hidden units <br>\n",
    "4) **Use vectorized code** <br>\n",
    "5) Use gradient descent for training and use reasonable stopping criteria for termination, such as stop when loss doesn't decrease much, use your judgement <br>\n",
    "6) Learning rate: 1e-7 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b22df9",
   "metadata": {
    "id": "20b22df9"
   },
   "source": [
    "You can read the mat file using scipy functionality:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e5e47f9b",
   "metadata": {
    "id": "e5e47f9b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bv/j678dvtd34bc5mtymgsd2c6w0000gn/T/ipykernel_43857/2156593710.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(torch.from_numpy(X_train).float(),dtype=torch.float64)\n",
      "/var/folders/bv/j678dvtd34bc5mtymgsd2c6w0000gn/T/ipykernel_43857/2156593710.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(torch.from_numpy(X_test).float(),dtype=torch.float64)\n",
      "/var/folders/bv/j678dvtd34bc5mtymgsd2c6w0000gn/T/ipykernel_43857/2156593710.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_train = torch.tensor(torch.from_numpy(Y_train).float(),dtype=torch.float64)\n",
      "/var/folders/bv/j678dvtd34bc5mtymgsd2c6w0000gn/T/ipykernel_43857/2156593710.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_test = torch.tensor(torch.from_numpy(Y_test).float(),dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "bodyfat = sio.loadmat('bodyfat_data.mat')\n",
    "X = bodyfat['X']\n",
    "Y = bodyfat['y']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=98,shuffle=True)\n",
    "X_train = torch.tensor(torch.from_numpy(X_train).float(),dtype=torch.float64)\n",
    "X_test = torch.tensor(torch.from_numpy(X_test).float(),dtype=torch.float64)\n",
    "Y_train = torch.tensor(torch.from_numpy(Y_train).float(),dtype=torch.float64)\n",
    "Y_test = torch.tensor(torch.from_numpy(Y_test).float(),dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "20c58d68-363a-45e5-9049-5839244eb27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchNeuralNet(nn.Module):\n",
    "    def __init__(self,inputSize, hiddenOne, hiddenTwo, outputSize):\n",
    "        super().__init__()\n",
    "        self.inputSize = inputSize\n",
    "        self.hiddenOne = hiddenOne\n",
    "        self.hiddenTwo = hiddenTwo\n",
    "        self.outputSize = outputSize\n",
    "        \n",
    "        torch.manual_seed(0)\n",
    "        \n",
    "        self.W1 = torch.normal(mean=torch.zeros(self.inputSize,self.hiddenOne),std=1/150*torch.ones(self.inputSize,self.hiddenOne)).double().requires_grad_()\n",
    "        self.b1 = torch.zeros(1,self.hiddenOne).double().requires_grad_()\n",
    "        \n",
    "        self.W2 = torch.normal(mean=torch.zeros(self.hiddenOne,self.hiddenTwo),std=1/150*torch.ones(self.hiddenOne, self.hiddenTwo)).double().requires_grad_()\n",
    "        self.b2 = torch.zeros(1,self.hiddenTwo).double().requires_grad_()\n",
    "        \n",
    "        self.W3 = torch.normal(mean=torch.zeros(self.hiddenTwo,self.outputSize),std=1/150*torch.ones(self.hiddenTwo,self.outputSize)).double().requires_grad_()\n",
    "        self.b3 = torch.zeros(1,self.outputSize).double().requires_grad_()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,X):\n",
    "        a1 = torch.mm(X,self.W1) + self.b1\n",
    "        z1 = self.relu(a1)\n",
    "        \n",
    "        a2 = torch.mm(z1,self.W2) + self.b2\n",
    "        z2 = self.relu(a2)\n",
    "        \n",
    "        yhat = torch.mm(z2,self.W3) + self.b3\n",
    "        return yhat\n",
    "    \n",
    "    def cost(self, yhat, y):\n",
    "        AveMSE = torch.mean( (y-yhat)**2 ) \n",
    "        return AveMSE\n",
    "   \n",
    "    def zero_grads(self):\n",
    "        with torch.no_grad():\n",
    "            self.W1.grad.zero_()\n",
    "            self.W2.grad.zero_()\n",
    "            self.W3.grad.zero_()\n",
    "            self.b1.grad.zero_()\n",
    "            self.b2.grad.zero_()\n",
    "            self.b3.grad.zero_()\n",
    "\n",
    "    def updateParams(self, lr):\n",
    "        with torch.no_grad():\n",
    "            self.W1 -= lr*self.W1.grad\n",
    "            self.W2 -= lr*self.W2.grad\n",
    "            self.W3 -= lr*self.W3.grad\n",
    "            self.b1 -= lr*self.b1.grad\n",
    "            self.b2 -= lr*self.b2.grad\n",
    "            self.b3 -= lr*self.b3.grad\n",
    "        \n",
    "    \n",
    "    def backward(self,yhat,y):\n",
    "        loss = self.cost(yhat,y)\n",
    "        loss.backward()\n",
    "        self.updateParams(0.0001)\n",
    "        self.zero_grads()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "e1b604ca-6cfa-4fb4-aab8-ac1f1f3b4a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 Loss: tensor(432.4668, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   1 Loss: tensor(432.3133, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   2 Loss: tensor(432.1598, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   3 Loss: tensor(432.0061, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   4 Loss: tensor(431.8524, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   5 Loss: tensor(431.6986, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   6 Loss: tensor(431.5446, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   7 Loss: tensor(431.3905, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   8 Loss: tensor(431.2363, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   9 Loss: tensor(431.0818, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   10 Loss: tensor(430.9271, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   11 Loss: tensor(430.7722, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   12 Loss: tensor(430.6170, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   13 Loss: tensor(430.4615, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   14 Loss: tensor(430.3057, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   15 Loss: tensor(430.1495, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   16 Loss: tensor(429.9930, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   17 Loss: tensor(429.8359, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   18 Loss: tensor(429.6784, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   19 Loss: tensor(429.5203, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   20 Loss: tensor(429.3616, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   21 Loss: tensor(429.2023, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   22 Loss: tensor(429.0423, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   23 Loss: tensor(428.8815, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   24 Loss: tensor(428.7198, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   25 Loss: tensor(428.5571, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   26 Loss: tensor(428.3933, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   27 Loss: tensor(428.2284, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   28 Loss: tensor(428.0622, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   29 Loss: tensor(427.8945, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   30 Loss: tensor(427.7251, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   31 Loss: tensor(427.5540, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   32 Loss: tensor(427.3809, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   33 Loss: tensor(427.2056, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   34 Loss: tensor(427.0277, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   35 Loss: tensor(426.8463, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   36 Loss: tensor(426.6593, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   37 Loss: tensor(426.4686, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   38 Loss: tensor(426.2737, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   39 Loss: tensor(426.0740, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   40 Loss: tensor(425.8689, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   41 Loss: tensor(425.6575, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   42 Loss: tensor(425.4390, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   43 Loss: tensor(425.2123, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   44 Loss: tensor(424.9762, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   45 Loss: tensor(424.7290, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   46 Loss: tensor(424.4691, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   47 Loss: tensor(424.1944, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   48 Loss: tensor(423.9022, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   49 Loss: tensor(423.5896, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   50 Loss: tensor(423.2529, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   51 Loss: tensor(422.8873, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   52 Loss: tensor(422.4871, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   53 Loss: tensor(422.0455, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   54 Loss: tensor(421.5534, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   55 Loss: tensor(420.9997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   56 Loss: tensor(420.3700, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   57 Loss: tensor(419.6462, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   58 Loss: tensor(418.8044, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   59 Loss: tensor(417.8136, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   60 Loss: tensor(416.6323, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   61 Loss: tensor(415.2049, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   62 Loss: tensor(413.4559, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   63 Loss: tensor(411.2811, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   64 Loss: tensor(408.5345, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   65 Loss: tensor(405.0091, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   66 Loss: tensor(400.4065, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   67 Loss: tensor(394.2906, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   68 Loss: tensor(386.0163, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   69 Loss: tensor(374.6226, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   70 Loss: tensor(358.6821, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   71 Loss: tensor(336.1264, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   72 Loss: tensor(304.1714, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   73 Loss: tensor(259.7819, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   74 Loss: tensor(201.7756, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   75 Loss: tensor(135.9070, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   76 Loss: tensor(79.5182, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   77 Loss: tensor(50.4128, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   78 Loss: tensor(43.5175, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch:   79 Loss: tensor(42.8808, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "\n",
      "Test:   79 Loss: tensor(59.0964, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = ScratchNeuralNet(2,64,16,1)\n",
    "epochs = 80\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    yhat = model(X_train)\n",
    "    model.backward(yhat,Y_train)\n",
    "    print(\"Epoch:\",\" \", epoch, \"Loss:\", model.cost(yhat,Y_train))\n",
    "\n",
    "\n",
    "yhatTest = model(X_test)\n",
    "model.backward(yhatTest, Y_test)\n",
    "print(\"\\nTest:\",\" \", epoch, \"Loss:\", model.cost(yhatTest,Y_test))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dc7937-5558-4608-802c-1e4a0609d1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f6c83e-d1b5-4fb1-a8fd-a6a4b07944c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a62e493-430c-4246-9be4-26f238cd6900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd151c82-cea0-4f9a-b237-91ea48b75dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47883a8-54ae-4817-924f-206ef146c47e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85313fca",
   "metadata": {
    "id": "85313fca"
   },
   "source": [
    "# Problem 2( 70 Points )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5502d743",
   "metadata": {
    "id": "5502d743"
   },
   "source": [
    "__Part 1 (Monthly Factor Data)__: Download the 5- factor Fama french factors monthly factors dataset from the below link:-\n",
    "    \n",
    "https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html\n",
    "\n",
    "Once you download the data, you will find that it contains a mix of annual and monthly factors and some texts at the top, so pd.read_csv will error out( try taking a look at csv file in text editor ). To get the monthly data, you can do the following:-\n",
    "\n",
    "df = pd.read_csv('F-F_Research_Data_5_Factors_2x3.csv', skiprows = 3).iloc[:-59,:]\n",
    "\n",
    "Use data from 1970-2010 as training data, 2010-2015 as validation data, 2015-2020 as a testing data. \n",
    "\n",
    "As you have seen from other courses, Factor models are heavily used in asset pricing as well as predicting future stock returns. For future stock return prediction, we typically have a seperate model for factor loadings. By factor loadings, we mean that coefficents of factor in the model. At time $t$ and by taking the dot product of 1) best estimation of factor loading for next time interval($t+1$) and 2) best estimate of factors at next time interval ($t+1$), one can come up with an estimation of future single stock return and create a long/short portfolio of stocks (long the stock with positive return prediction and short the ones that have negative predicted returns). However, most of these predictive models simply use the current value of the factors as the best guess for the factor value as time $t+1$. An alternative choice is using moving average of factor values as an estimate for the value of the factor at next time step.\n",
    "\n",
    "In this question, you are asked to analyze whether using a time series model (RNN/GRU/LSTM) would be able to give us better estimate for the value of the future factors values. To be more concrete, you need to try some recurrent neural networks and  use past values of the factors as input and the value of the factors at next time interval as the target. You can try different design for the network and its inputs, for example:\n",
    "\n",
    "- The input is 1-dimensional (the future value of each factor depends only on its own past).\n",
    "- The imput is 5-dimensional (the future value of each factor depends on history of all factors).\n",
    "\n",
    "You can either train 5 different neural networks to predict each of the 5 factors, or use one neural network to predict 5 factors at once\n",
    "\n",
    "Use data from 1970-2010 as training data, 2010-2015 as validation data, 2015-2020 as a testing data. You should tune the parameters using validation set only. After you have tuned the hyperparameters, use the model to make predictions on testing data and report the accuracy and performance of model. Compare the performance of the model with the two benchmarks mentioned above (last known value or moving average of previous values). Choose the window size for moving average as you desire\n",
    "\n",
    "__Part 2__ As you might have learnt from past courses, all these 5 factors are tradable portfolios by construction (for example SMB factor is a tradable portfolio loging small Caps and shorting Big Caps). Using the predictive model of previous step, design a trading strategy (for example, long the factor with highest estimated return and short the factor with smallest estimated return, or any other strategy of your choice). Calculate total return, sharpe ratio for the out of sample period.\n",
    "\n",
    "To be more precise, you can choose a starting cash amount such as 100,000 dollars, and see the performance of your trading strategy on this amount\n",
    "\n",
    "Suppose the factor predicted at time step is $F_{t+1}$, you can calculate the percentage return as $\\frac{F_{t+1} - F_{t}}{F_{t}}$. You can similarly calculate the return of true factors in testing data\n",
    "\n",
    "To be more precise, if the return vector of factors after prediction from neural network is $[0.1, 0.05, 0.07, 0.08, -0.09]$, then we will long the first factor and short the fifth factor. If the return vector of factors our testing dataset is $[0.02, 0.08, 0.54, 0.09, 0.05]$, then our PNL will be  $100,000*(-0.02 + 0.05)$ which is equal to $3000$\n",
    "\n",
    "You can sum this PNL across various time steps by calculating and that will give you the performace of trading strategy on entire testing data( out of sample data)\n",
    "\n",
    "As you can see, since the factors are tradables itself, we don't need to worry about factor loadings in calculating the PnL. You can experiment with any trading strategy of your choice based on these factors\n",
    "\n",
    "\n",
    "__Part 3__  Repeat the previous two steps by woking on daily factor data (you can download it from the same url) and perform a comparison between the two approaches\n",
    "\n",
    "When use download daily data, you will also find that it contains text in the beginning. Just set the parameter skiprows to 3 in pd.read_csv, and it will work fine and then you can proceed along similar lines\n",
    "\n",
    "Feel free to use your judgement on this problem and make assumptions anywhere you get stuck, just mention that in the notebook. The design of the problem is made open-ended intentionally\n",
    "\n",
    "__Feel free to use google colab for Keras or Pytorch.__<br>\n",
    "__Don't forget to use a lot of regularization to minimize the risk of overfitting__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "a09e068d-1653-4d84-b6e6-ecde007870a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('F-F_Research_Data_5_Factors_2x3.csv', skiprows = 3).iloc[:-59,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "b47f1315-2629-43fd-8791-b10cf7136745",
   "metadata": {},
   "outputs": [],
   "source": [
    "index1 = df[df[\"Unnamed: 0\"]==\"197001\"].index[0]\n",
    "index2 = df[df[\"Unnamed: 0\"]==\"200912\"].index[0]\n",
    "index3 = df[df[\"Unnamed: 0\"]==\"201512\"].index[0]\n",
    "index4 = df[df[\"Unnamed: 0\"]==\"202012\"].index[0]\n",
    "\n",
    "trainFeatures = df[[\"SMB\",\"HML\",\"RMW\",\"CMA\",\"RF\"]].loc[index1:index2]\n",
    "validationFeatures = df[[\"SMB\",\"HML\",\"RMW\",\"CMA\",\"RF\"]].loc[index2+1:index3]\n",
    "testFeatures = df[[\"SMB\",\"HML\",\"RMW\",\"CMA\",\"RF\"]].loc[index3+1:index4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "fd3cdb35-e263-45d8-9665-49eec400cd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape =  (478, 1, 5)\n",
      "y_train.shape =  (478, 5)\n"
     ]
    }
   ],
   "source": [
    "def load_data(df, look_back):\n",
    "    data_raw = df.values # convert to numpy array\n",
    "    data = []\n",
    "    \n",
    "    # create all possible sequences of length look_back\n",
    "    for index in range(data_raw.shape[0] - look_back): \n",
    "        data.append(data_raw[index: index + look_back])\n",
    "    \n",
    "    data = np.array(data)\n",
    "    \n",
    "    x_train = data[:,:-1,:] #first (lookback - 1) elements\n",
    "    y_train = data[:,-1,:] #last element which is to be predicted based on x_train\n",
    "\n",
    "    return x_train, y_train\n",
    "\n",
    "look_back = 2 # sequence length\n",
    "x_train, y_train = load_data(trainFeatures, look_back)\n",
    "x_val, y_val = load_data(validationFeatures, look_back)\n",
    "x_test, y_test = load_data(testFeatures, look_back)\n",
    "print('x_train.shape = ',x_train.shape)\n",
    "print('y_train.shape = ',y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "89342dda-07e4-4036-b27d-039e94e3fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(np.asfarray(x_train)).type(torch.Tensor)\n",
    "x_test = torch.from_numpy(np.asfarray(x_test)).type(torch.Tensor)\n",
    "x_val = torch.from_numpy(np.asfarray(x_val)).type(torch.Tensor)\n",
    "y_train = torch.from_numpy(np.asfarray(y_train)).type(torch.Tensor)\n",
    "y_test = torch.from_numpy(np.asfarray(y_test)).type(torch.Tensor)\n",
    "y_val = torch.from_numpy(np.asfarray(y_val)).type(torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "id": "885497f6-b358-4247-bc81-56ffc1a49bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 5 # there is only single value at each node\n",
    "hidden_dim = 2\n",
    "num_layers = 2 #one LSTM stacked on top of each other\n",
    "output_dim = 5 #this is a regression problem now\n",
    "\n",
    "# Here we define our model as a class\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.n_outputs = output_dim\n",
    "\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True,dropout = 0.5)\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        self.batch_size = x.size(0)\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "\n",
    "        out, _ = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        final_out = self.fc(out[:,-1,:]) \n",
    "        final_out = final_out.view(-1,self.n_outputs)\n",
    "\n",
    "        return final_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "id": "4319c52d-c101-44a7-a425-78b7d700f531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(5, 2, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=2, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "id": "7f83e283-4e32-4b10-ab01-7d753abbc194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  100 MSE:  5.986821174621582\n",
      "Epoch  100 Val MSE:  2.5775551795959473\n",
      "\n",
      "Epoch  200 MSE:  5.904903888702393\n",
      "Epoch  200 Val MSE:  2.5741794109344482\n",
      "\n",
      "Epoch  300 MSE:  5.841479778289795\n",
      "Epoch  300 Val MSE:  2.5899641513824463\n",
      "\n",
      "Epoch  400 MSE:  5.787330627441406\n",
      "Epoch  400 Val MSE:  2.633690595626831\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "hist = np.zeros(num_epochs)\n",
    "hist2 = np.zeros(num_epochs)\n",
    "# Number of steps to unroll\n",
    "seq_dim =look_back-1  \n",
    "\n",
    "for t in range(num_epochs):\n",
    "    \n",
    "    # Forward pass\n",
    "    y_train_pred = model(x_train)\n",
    "\n",
    "    loss_val = loss(y_train_pred, y_train)\n",
    "\n",
    "    if t % 100 == 0 and t !=0:\n",
    "        print(\"\\nEpoch \", t, \"MSE: \", loss_val.item())\n",
    "        print(\"Epoch \",t,\"Val MSE: \",loss(model(x_val),y_val).item())\n",
    "\n",
    "    hist[t] = loss_val.item()\n",
    "    hist2[t] = loss(model(x_val),y_val).item()\n",
    "    # Zero out gradient, else they will accumulate between epochs\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss_val.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "id": "b2fa2c93-0649-44ad-b3b1-3cac5c3dc6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'Number of Epochs'),\n",
       " Text(0, 0.5, 'Average Mean-Square Error'),\n",
       " Text(0.5, 1.0, 'Error vs. Num Epochs')]"
      ]
     },
     "execution_count": 813,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "hist = pd.DataFrame(hist)#\n",
    "hist.rename(columns={0:\"Training Error\"},inplace=True)\n",
    "hist2 = pd.DataFrame(hist2)\n",
    "hist2.rename(columns={0:\"Validation Error\"},inplace = True)\n",
    "\n",
    "fig = sns.lineplot(data = hist)\n",
    "fig = sns.lineplot(data = hist2,palette=['r'])\n",
    "fig.set(xlabel=\"Number of Epochs\",ylabel=\"Average Mean-Square Error\",title = \"Error vs. Num Epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55943fb6-ed28-419c-a9d7-204c6946eed2",
   "metadata": {},
   "source": [
    "I now consider my model fully optimized. my methods for regulating over fitting are setting the dropout probability to 50%. I also have reduced the number of hidden dimension to just 2 and the sequence length to 1. I am genuinely suprised by the model parameters, but the larger my model tended to get in any of these parameter, the more quickly I saw evidence of severe overfitting. Now I will compare this LSTM model to baseline methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "id": "fa4d18e4-d082-4ca4-9b93-0afa389d6b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The model has loss of 5.135113716125488 over the TEST SET\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nThe model has loss of {loss(model(x_test),y_test).item()} over the TEST SET\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "id": "46a8b422-1749-401e-aae4-82f858b3f573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The loss on the TEST SET from using the factors at time t as a proxy for the factors at time t+1 gives us an average MSE of 8.750276565551758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lastVal = x_test[:,-1,:]\n",
    "lastSeqPredictionLoss = loss(lastVal,y_test).item()\n",
    "print(f'\\nThe loss on the TEST SET from using the factors at time t as a proxy for the factors at time t+1 gives us an average MSE of {lastSeqPredictionLoss}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "id": "cccb322e-2dae-426e-a6c7-e1d7c9366dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The loss on the TEST SET from using a moving average over the last 5 periods as a proxy for the factors at time t+1 gives us an average MSE of 6.193720536014496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_X_Test = np.array(x_test)\n",
    "MALength = 5\n",
    "MA = np.zeros((temp_X_Test.shape[0]-MALength,5))\n",
    "\n",
    "for i in range(temp_X_Test.shape[0]-MALength):\n",
    "    mySlice = temp_X_Test[i:i+MALength]\n",
    "    MA[i,:] = np.average(mySlice,axis=0)\n",
    "\n",
    "temp = torch.from_numpy(MA)\n",
    "MAPredLoss = loss(temp,y_test[MALength:]).item()\n",
    "print(f'\\nThe loss on the TEST SET from using a moving average over the last 5 periods as a proxy for the factors at time t+1 gives us an average MSE of {MAPredLoss}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbd249d-4af1-4482-b729-14e4a0b6daaa",
   "metadata": {},
   "source": [
    "Overall, we can see that our simple LSTM network has improved the performance quite considerably from both baseline methods. Given the simplicity of the model I believe this absolutely warrants the use of this model over the baseline methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf759cdc-344a-4c34-817d-9cebaa9e3738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a51fcc-2a63-4b1f-b46c-e8ee104f85b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e4fc1-66fd-4741-886e-ea4d24200cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f401675e-341b-428a-82ca-df486927aeb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1885853-8f1e-48e8-ac03-804b38aac89e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ad9e0-d9e0-4012-b761-4b0f3e4afc93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feadaa0d-8338-4704-a7fe-423b4cc8eb04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4535da-8fc8-4994-ae69-1f5d083a4906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d03cce-ff50-467a-ba90-e44614a36d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd3dcf3-5b05-40da-b307-d2490963146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STEPS = 4 #sequence length\n",
    "N_INPUTS = 5\n",
    "N_NEURONS = 20\n",
    "N_OUTPUTS = 5\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d005bd-79e9-4889-bd12-4079ade430c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, batch_size, n_steps, n_inputs, n_neurons, n_outputs):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.n_neurons = n_neurons\n",
    "        self.batch_size = batch_size\n",
    "        self.n_steps = n_steps #Sequence Length\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs #10\n",
    "        \n",
    "        self.basic_rnn = nn.RNN(self.n_inputs, self.n_neurons) \n",
    "        \n",
    "        self.FC = nn.Linear(self.n_neurons, self.n_outputs)\n",
    "        \n",
    "    def init_hidden(self,):\n",
    "        # (num_layers, batch_size, n_neurons)\n",
    "        return (torch.zeros(1, self.batch_size, self.n_neurons))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # transforms X to dimensions: n_steps X batch_size X n_inputs\n",
    "        X = X.permute(1, 0, 2) \n",
    "        \n",
    "        self.batch_size = X.size(1)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        # out => n_steps, batch_size, n_neurons (hidden states for each time step)\n",
    "        # self.hidden => 1, batch_size, n_neurons (final state from each lstm_out)\n",
    "        out, self.hidden = self.basic_rnn(X, self.hidden)      \n",
    "        final_out = final_out.view(-1,self.n_outputs)\n",
    "        \n",
    "        return  F.log_softmax(final_out, dim = 1)# batch_size X n_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb582fc3-7926-48bf-9fb6-8d238109442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "model = RNN(BATCH_SIZE, N_STEPS, N_INPUTS, N_NEURONS, N_OUTPUTS)\n",
    "output = model(images.view(-1, 28,28))\n",
    "print(output[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd16468-addf-42b1-b578-d48c63f9d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = ImageRNN(BATCH_SIZE, N_STEPS, N_INPUTS, N_NEURONS, N_OUTPUTS)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def get_accuracy(output, labels, batch_size):\n",
    "    \n",
    "    corrects = (torch.max(output, 1)[1] == labels).sum()\n",
    "    accuracy = 100.0 * corrects/batch_size\n",
    "    return accuracy.item(). #accuracy is a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b89a7-bbd4-45ed-a48e-3936202bf9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(N_EPOCHS):  # loop over the dataset multiple times\n",
    "    train_running_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for i, data in enumerate(trainloader):\n",
    "         # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # reset hidden states\n",
    "        model.hidden = model.init_hidden() \n",
    "        \n",
    "        # get the inputs\n",
    "        X, labels = data\n",
    "        X = X.view(-1, 28,28) #batch_size*image_dimension \n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(X)\n",
    "\n",
    "        loss_val = loss(outputs, labels)\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_running_loss += loss_val.detach().item()\n",
    "        train_acc += get_accuracy(outputs, labels, BATCH_SIZE)\n",
    "         \n",
    "    model.eval()\n",
    "    print('Epoch:  %d | Loss: %.4f | Train Accuracy: %.2f' \n",
    "          %(epoch, train_running_loss / i, train_acc/i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0780c9f-5442-4f5b-91e5-ffe90036dc15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d1867-98b3-493e-a98b-aff84edda37b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21856b6-d555-4c2a-9868-264b5a48306b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
